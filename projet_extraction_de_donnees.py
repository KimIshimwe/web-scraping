# -*- coding: utf-8 -*-
"""Projet_extraction_de_donnees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/187PyDNktHp3tKS-3EL5spHsUcreL2oCi

##Code pour l'extraction des fichiers  films en local
"""

import os
from bs4 import BeautifulSoup
import pandas as pd
from requests import get
import matplotlib.pyplot as plt

titres=[]
dates = []
genres = []
resumes = []
id_ind = []
nom_individus = []
prenom_individus = []
id_film = []
id_realisateur = []
id_acteur = []
id_film_act = []
id_ind_act = []

role = []
Nationalite = []
DateSortie = []

i = 0

chemin = '/content/LesSortiesCinemas'


for filename in os.listdir(chemin):
  fichier = os.path.join(chemin,filename)
  with open(fichier,"r") as f:

    soup = BeautifulSoup(f, "html.parser")
    # titre film

    span_Titre = soup.find_all("span", {"class": "title"})
    span_Titre
    for p_titre in span_Titre:
      titre = p_titre.find()


    if titre is not None:
      mes_titres = titre.text.replace("\n       ","")
      titres.append(mes_titres)
    else:
      titres.append("N/A")

    # Annee de sortie

    span_dates = soup.find_all("span", {"class": "updated"})

    for p_date in span_dates:
      temp = p_date.text.split(',')[0]
      annee = temp.split('/')[-1]
      # date de sortie
      temp1 = temp.split(":")[1]
      date_complet = temp1.split(" ")[1]
      DateSortie.append(date_complet)
      dates.append(annee)

  # genre du film

    div_content = soup.find("div", {"class": "content"})
    for p_genre in div_content:
      texte_p = div_content.find("p").text
      div_duree_genre = texte_p.split("-")
      div_duree_genre = div_duree_genre[0].strip() # Utilisez strip() pour supprimer les espaces indésirables
    if div_duree_genre is not None:
      genre = div_duree_genre.split('(')[0]
      genres.append(genre)
    else:
      genres.append("N/A")


  # resume

    p_resumes = soup.find("div",{"class":"content"}).find_all('p')[0] # 1er paragraphe de la classe
    for resume in p_resumes:

      if resume is not None:
        mes_resumes = resume.text.split("-")[1].replace("\n        ","")
        resumes.append(mes_resumes)
      else:
        resumes.append(None)


     # identifiant du film
    p_id_titre = soup.find("span", {"class":"title"}).find('a')
    id_html = str(p_id_titre).split("=")[-1:]
    id_titre =  id_html[0].split(".")[0]
    id_film.append(int(id_titre))


     # identifiant des réalisateurs pour la classe films
    p_individu = soup.find("div",{"class": "content"}).find_all('p')[1] # 2ème paraphe de la classe content
    p_realisateurs = p_individu.find_all('a')
    if len(str(p_realisateurs).split("href")) > 1:
      temp = str(p_realisateurs).split("href")[1] # l'url
      temp1 = temp.split(".")[2]
      id_real = temp1.split("=")[-1]
      id_realisateur.append(int(id_real))
    else:
      id_real = None
      id_realisateur.append(id_real)



    #identifiant indvidu
    p_individu = soup.find("div",{"class": "content"}).find_all('p')[1] # 2ème paraphe de la
    liste_individu = str(p_individu).split("href")
    for elt in liste_individu:
      if "=" in elt:
        temp = elt.split('=')[2]
        id = temp.split('.')[0]
        id_ind.append(int(id))



  # Nom et prénoM des individus
    p_acteur = soup.find("div",{"class":"content"}).find_all("p")[1] # on cherche sur le deuxième paragraphe

    for act in p_acteur.find_all('a'): # afficher toutes les balises 'a' du paragraphe à partir de la 2ème

      if act is not None :
        temp = act.text.split(' ')
        if len(temp) > 1: # s'il y a le nom et le prénom

          nom_individus.append(temp[1])
          prenom_individus.append(temp[0])

        elif len(temp) == 1: # s'il n'y a que le nom
          nom_individus.append(temp)
          prenom_individus.append(None)

      else:
        nom_individus.append(None)
        prenom_individus.append(None)

        # Nationalité des individus
    p_nationalite = soup.find("div",{"class": "content"}).find_all('p')[1]
    for url in p_nationalite.find_all('a', href = True):
     # récupérer le contenu du lien liens
      html = get(url['href'])
      html.text
      soup = BeautifulSoup(html.text, "html.parser")
      p_nationalite = soup.find("div", {"class": "dark-grey"})
      if p_nationalite is not None:
        Nationalite.append(p_nationalite.text)
      else:
        Nationalite.append(None)

  for id in id_ind:
    if id not in id_realisateur and id not in id_acteur:  #  un individu est soit un acteur soit un réalisateur
      id_acteur.append(int(id))
      id_film_act.append(int(id_titre)) # on ajoute pour chaque film ses participants (acteurs et réalisateurs)



# Nettoyage de la liste Nationalite
for elt in Nationalite:
  if (elt == "43 ans") or (elt == "65 ans"):
    elt = None

"""##Code pour la création des dataframes"""

# Création des dataframes
import pandas as pd
df_individus = pd.DataFrame({"NumInd": id_ind ,"Nom": nom_individus, "Prenom": prenom_individus, "Nationalite": Nationalite})
df_individus.reset_index(drop = True, inplace=True)
#df_individus = df_individus.drop_duplicates()
df_individus.to_csv("BD601_INDIVIDUS.csv")


df_films = pd.DataFrame({"NumFilm":id_film,"NumInd": id_realisateur,"Titre": titres, "Genre": genres,"Date de sortie": DateSortie, "Annee": dates ,"Resume": resumes })
df_films = df_films.drop_duplicates()
df_films.to_csv("BD601_FILMS.csv")


df_acteurs = pd.DataFrame({"NumInd": id_acteur, "NumFilm": id_film_act})
df_acteurs = df_acteurs.drop_duplicates()
df_acteurs.to_csv("BD601_ACTEURS.csv")

"""##Code pour les fichiers distants (qui n' pas vraiment marché)"""

titres=[]
dates = []
genres = []
resumes = []
id_ind = []
nom_individus = []
prenom_individus = []
id_film = []
id_realisateur = []
id_acteur = []
id_film_act = []
id_ind_act = []

role = []
Nationalite = []
DateSortie = []
liens = []

i = 0

# Aller vers le lien des 3000 fichiers
url = "https://mi-phpmut.univ-tlse2.fr/~catherine.comparot/LesSortiesCinema/"
articles = requests.get(url)
soup = BeautifulSoup(articles.text)

# Trouvers les liens des films
for lien in soup.find_all('a'):
  if lien.get('href').endswith('.html'):
    liens.append(url + lien.get('href'))

for lien in liens:
  res = requests.get(lien)
  soup = BeautifulSoup(res.text, "html.parser")



    # titre film

  span_Titre = soup.find_all("span", {"class": "title"})
  span_Titre
  for p_titre in span_Titre:
    titre = p_titre.find()


  if titre is not None:
    mes_titres = titre.text.replace("\n       ","")
    titres.append(mes_titres)
  else:
    titres.append("N/A")

    # Annee de sortie
  if soup.find_all("span", {"class": "updated"}) is not None:

    span_dates = soup.find_all("span", {"class": "updated"})

    for p_date in span_dates:
      temp = p_date.text.split(',')[0]
      if temp is not None:
        annee = temp.split('/')[-1]
        dates.append(annee)

      else:
        dates.append(None)
      # date de sortie
      temp1 = temp.split(":")[1]
      if temp1 is not None:
        date_complet = temp1.split(" ")[1]
        DateSortie.append(date_complet)
      else:
        date_complet.append(None)







  # genre du film

  div_content = soup.find("div", {"class": "content"})
  if div_content is not None: # si le contenu a un résumé
    for p_genre in div_content:
      if div_content.find("p") is not None:
        texte_p = div_content.find("p").text
        div_duree_genre = texte_p.split("-")
        div_duree_genre = div_duree_genre[0].strip() # Utilisez strip() pour supprimer les espaces indésirables

    if div_duree_genre is not None:
      genre = div_duree_genre.split('(')[0]
      genres.append(genre)
    else:
      genres.append(None)

  #resume
  if soup.find("div",{"class":"content"}) is not None:
    p_resumes = soup.find("div",{"class":"content"}).find_all('p') # 1er paragraphe de la classe
    if p_resumes: # s'il y a au moins un paragraphe dans la classe
      for resume in p_resumes[0]:
        if resume is not None:
          mes_resumes = resume.text.split("-")[1].replace("\n        ","")
          resumes.append(mes_resumes)
        else:
          resumes.append(None)


     # identifiant du film
  if soup.find("span", {"class":"title"}) is not None:

    p_id_titre = soup.find("span", {"class":"title"}).find('a')
    id_html = str(p_id_titre).split("=")[-1]
    id_titre =  id_html.split(".")[0]
    id_film.append(id_titre)
  else:
    id_film.append(None)

     # identifiant des réalisateurs pour la classe films
  if soup.find("div",{"class": "content"}) is not None:
    if len(soup.find("div",{"class": "content"}).find_all('p')) > 1:

      p_individu = soup.find("div",{"class": "content"}).find_all('p')[1] # 2ème paraphe de la classe content
      p_realisateurs = p_individu.find_all('a')
      if len(str(p_realisateurs).split("href")) > 1:
        temp = str(p_realisateurs).split("href")[1] # l'url
        temp1 = temp.split(".")[2]
        id_real = temp1.split("=")[-1]
        id_realisateur.append(int(id_real))
      else:
        id_real = None
        id_realisateur.append(id_real)



    #identifiant indvidu
  if soup.find("div",{"class": "content"}) is not None:
    if len(soup.find("div",{"class": "content"}).find_all('p')) > 1:
      p_individu = soup.find("div",{"class": "content"}).find_all('p')[1] # 2ème paraphe de la
      liste_individu = str(p_individu).split("href")
      for elt in liste_individu:
        if "=" in elt:
          temp = elt.split('=')[2]
          id = temp.split('.')[0]
          id_ind.append(int(id))
    else:
      id_ind.append(None)


  # Nom et prénom des individus
  if  soup.find("div",{"class":"content"}) is not None:
    if len(soup.find("div",{"class":"content"}).find_all("p")) > 1:

      p_acteur = soup.find("div",{"class":"content"}).find_all("p")[1] # on cherche sur le deuxième paragraphe

      for act in p_acteur.find_all('a'): # afficher toutes les balises 'a' du paragraphe à partir de la 2ème
        if act is not None :
          temp = act.text.split(' ')
          if len(temp) > 1: # s'il y a le nom et le prénom
            nom_individus.append(temp[1])
            prenom_individus.append(temp[0])

          elif len(temp) == 1: # s'il n'y a que le nom
            nom_individus.append(temp)
            prenom_individus.append(None)
          else:
            nom_individus.append(None)
            prenom_individus.append(None)

 # identifiants des acteurs
  for id in id_ind:
    if id not in id_realisateur and id not in id_acteur:  #  un individu est soit un acteur soit un réalisateur
      id_acteur.append(int(id))
      id_film_act.append(int(id_titre))


# Nettoyage de la liste Nationalite
  #for elt in Nationalite:
   # if (elt == "43 ans") or (elt == "65 ans"):
     # elt = None

while len(dates) != len(genres):
  dates.append(None)
while len(DateSortie) != len(genres):
  DateSortie.append(None)

# Création des dataframes
import pandas as pd
df_individus = pd.DataFrame({"NumInd": id_ind ,"Nom": nom_individus, "Prenom": prenom_individus})
df_individus.reset_index(drop = True, inplace=True)
df_individus.to_csv("BD601_INDIVIDUS.csv")

df_films = pd.DataFrame({"NumFilm":id_film,"NumInd": id_realisateur,"Titre": titres, "Genre": genres,"Date de sortie": DateSortie, "Année": dates,"Resume": resumes })
df_films.to_csv("BD601_FILMS.csv")

df_acteurs = pd.DataFrame({"NumInd": id_acteur, "NumFilm": id_film_act})
df_acteurs.to_csv("BD601_ACTEURS.csv")

"""##Code pour extraire la nationalité avec les fichiers distants"""

Nationalite = []
DateSortie = []
liens = []

i = 0

# Aller vers le lien des 3000 fichiers
url = "https://mi-phpmut.univ-tlse2.fr/~catherine.comparot/LesSortiesCinema/"
articles = requests.get(url)
soup = BeautifulSoup(articles.text)

# Trouvers les liens des films
for lien in soup.find_all('a'):
  if lien.get('href').endswith('.html'):
    liens.append(url + lien.get('href'))

for lien in liens:
  res = requests.get(lien)
  soup = BeautifulSoup(res.text, "html.parser")
   # Nationalité des individus
  if soup.find("div",{"class": "content"}) is not None:
    if len(soup.find("div",{"class": "content"}).find_all('p')) >=  1:
      p_nationalite = soup.find("div",{"class": "content"}).find_all('p')[1]
      for url in p_nationalite.find_all('a', href = True):
     # récupérer le contenu du lien liens
        html = get(url['href'])
        html.text
        soup = BeautifulSoup(html.text, "html.parser")
        p_nationalite = soup.find("div", {"class": "dark-grey"})
        if p_nationalite is not None:
          Nationalite.append(p_nationalite.text)
        else:
          Nationalite.append(None)
    else:
      Nationalite.append(None)
  else:
    Nationalite.append(None)

"""##Code pour la visualisation"""

import pandas as pd
import matplotlib.pyplot as plt

# Nombre de films par mois
df_sorties_par_mois = pd.read_csv("sorties_par_mois.csv", sep  = ",")
print(df_sorties_par_mois)
plt.plot(df_sorties_par_mois["Mois"],df_sorties_par_mois["Nombre_de_sorties"])
plt.title("Nombre de sorties par mois")
plt.xlabel("Mois")
plt.ylabel("Nombre de films")

# Nombre de films par année
df_sorties_par_annee = pd.read_csv("nbre_sorties_annee.csv", sep  = ",")
print(df_sorties_par_annee.describe())
plt.plot(df_sorties_par_annee["Annee"], df_sorties_par_annee["Nombre_de_sorties"])
plt.title("Nombre de sorties par année")
plt.xlabel("Années")
plt.ylabel("Nombre de films")
#plt.xticks(['2016','2017', '2018', '2019', '2020'])
plt.xticks(df_sorties_par_annee["Annee"][::1]) # saut d'une unité sur l'axe des abscisses

# Nombre de films par genre
df_genre = pd.read_csv("NbreSorties_genre.csv", sep = ",")
df_genre
df_filtre = df_genre[df_genre['nbre_de_sorties'] > 6]
plt.bar(df_filtre["genre"], df_filtre["nbre_de_sorties"])

plt.xticks(fontsize = 8)
df_genre
plt.title("Nombre de sorties par genre")
plt.xlabel("Genre")
plt.ylabel("Nombre de sorties")

df_individus_nationalite = pd.read_csv("Nbre_Individus_Nationalite.csv", sep  = ",")
df_individus_nationalite["Nationalite"]
print(df_individus_nationalite)
